{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mateg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('french')\n",
    "stop_words.extend(['enfin','jusqu','part','peu','them','souvent','dedie','penser','presenton','eter','acc','esser','ainsi','non','partir','oeuvre','si','ete','cet', 'article', 'faire', 'bon', 'permettre', 'permet', 'proposer', 'proposons', 'obtenir', 'obtenons','tel', 'présenter', 'présentons', 'quel','face','mettre','basee','issu','gerer','but','etre','source','tre','pre','lie','sous', 'forme', 'faire', 'être', 'ete', 'donner', 'partie', 'étape', 'utiliser', 'utilisons', 'être', 'plus', 'fin', 'contexte'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Link</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adolfo_Couve</td>\n",
       "      <td>https://fr.wikipedia.org/wiki/Adolfo_Couve</td>\n",
       "      <td>['id=\"Biographie\"&gt;Biographie[modifier', '|', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Boaz_Arad</td>\n",
       "      <td>https://fr.wikipedia.org/wiki/Boaz_Arad</td>\n",
       "      <td>['id=\"Biographie\"&gt;Biographie[modifier', '|', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ralph_Barton</td>\n",
       "      <td>https://fr.wikipedia.org/wiki/Ralph_Barton</td>\n",
       "      <td>['id=\"Biographie\"&gt;Biographie[modifier', '|', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mark_Campos</td>\n",
       "      <td>https://fr.wikipedia.org/wiki/Mark_Campos</td>\n",
       "      <td>['id=\"Biographie\"&gt;Biographie[modifier', '|', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Michel_Caron_(artiste)</td>\n",
       "      <td>https://fr.wikipedia.org/wiki/Michel_Caron_(ar...</td>\n",
       "      <td>['id=\"Biographie\"&gt;Biographie[modifier', '|', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Name                                               Link  \\\n",
       "0            Adolfo_Couve         https://fr.wikipedia.org/wiki/Adolfo_Couve   \n",
       "1               Boaz_Arad            https://fr.wikipedia.org/wiki/Boaz_Arad   \n",
       "2            Ralph_Barton         https://fr.wikipedia.org/wiki/Ralph_Barton   \n",
       "3             Mark_Campos          https://fr.wikipedia.org/wiki/Mark_Campos   \n",
       "4  Michel_Caron_(artiste)  https://fr.wikipedia.org/wiki/Michel_Caron_(ar...   \n",
       "\n",
       "                                             Content  \n",
       "0  ['id=\"Biographie\">Biographie[modifier', '|', '...  \n",
       "1  ['id=\"Biographie\">Biographie[modifier', '|', '...  \n",
       "2  ['id=\"Biographie\">Biographie[modifier', '|', '...  \n",
       "3  ['id=\"Biographie\">Biographie[modifier', '|', '...  \n",
       "4  ['id=\"Biographie\">Biographie[modifier', '|', '...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"../data/Suicides_wikipedia_scrapping_fr.csv\", encoding=\"utf-8-sig\", delimiter=\",\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Biography = data.Content.values.tolist()\n",
    "# Remove Emails\n",
    "Biography = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in Biography]\n",
    "\n",
    "# Remove new line characters\n",
    "Biography = [re.sub('\\s+', '', sent) for sent in Biography]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "Biography = [re.sub(\"\\'\", \"\", sent) for sent in Biography]\n",
    "\n",
    "# Remove the commas\n",
    "Biography = [re.sub(\",\", \" \", sent) for sent in Biography]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[id=\"Biographie\">Biographie[modifier | modifier le code]Carrière artistique[modifier | modifier le code]Artiste multidisciplinaire (peinture  sculpture  photographie  vidéo)  il enseigne dans différentes écoles de Tel Aviv depuis 1993. Il est récompensé par le prix du Ministère de \"lÉducation\" et de la Culture (2003) et le prix du musée \"dart\" de Petah Tikva (2006)[2].Ses œuvres sont présentées dans différents musées  dont le musée \"dIsraël\" à Jérusalem[2].Mort[modifier | modifier le code]Le 1er février 2018  le site \"dinformations\" Mako révèle \"quil\" est visé par une enquête concernant des relations sexuelles \"quil\" aurait eues avec des élèves mineures. Le lendemain  il est retrouvé mort chez lui  apparemment suicidé[3].<span]'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Biography[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Biography_clean_brackets = ['']*len(Biography)\n",
    "for i in range(len(Biography)):\n",
    "    for j in range(69, len(Biography[i]) - 7):\n",
    "        Biography_clean_brackets[i]+=Biography[i][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Kansas City (Missouri)  né \"dun\" père avocat et \"dune\" mère artiste-peintre  Ralph Barton se révèle très jeune doué pour le dessin.Il débute comme caricaturiste pour The Kansas City Star et The Kansas City Journal-Post  puis abandonne ses études  et part pour Chicago en 1909 tenter \"lInstitut\" \"dart\" de Chicago. Il renonce à vivre dans cette ville et revient à Kansas City pour se remarier avec Marie Jennings  sa première épouse.En 1910  il commence à contribuer au magazine satirique Puck ; il déménage à New York avec sa femme et leur fille  poursuit son travail pour Puck et \"McCalls.\" Il loue ensuite un atelier \"quil\" partage avec le peintre Thomas Hart Benton \"cest\" Benton qui sert de premier modèle aux caricatures de Barton qui vont connaître un certain succès.En 1915  Puck décide \"denvoyer\" Barton en mission de reportage dans le Paris en guerre il \"sattache\" au pays  à la Ville lumière  et durant toute sa vie  y fera de fréquents séjours. Le 30 juin 1927  il est nommé chevalier de la Légion \"dhonneur\" à titre étranger  en qualité \"quartiste-dessinateur.Durant\" les années 1910-1920  Barton réussit à croquer la plupart des célébrités de son temps. Son dernier croquis fut Charlie Chaplin. \"Lun\" de ses dessins les plus célèbres  « When the Five \"OClock\" Whistle Blows in Hollywood » paru dans Vanity Fair en septembre 1921 et représente 139 personnalités \"dHollywood \" réunis en sorte de « revue » avec lever de rideau  immortalisé sur le Hollywood Boulevard. Il travaille également pour The New Yorker à partir de 1924  en tant que conseiller artistique  et contribue également au \"Colliers\" Weekly  Judge et \"Harpers\" Bazaar  magazine dans lequel sort en feuilleton à partir de 1925 le roman comique \"dAnita\" Loos  Gentlemen Prefer Blondes.Sur \"linsistance\" de son ami Chaplin  il réalisa en 1926 un seul et unique court-métrage  Camille  une adaptation libre de La Dame aux camélias  avec  entre autres  Paul Robeson  Ethel Barrymore  et Sinclair Lewis  et où \"lon\" peut remarquer Paul Claudel  Sacha Guitry  etc.  un casting très impressionnant \"dartistes\" de \"lépoque \" français  allemands et américains.Au pic de sa popularité  admiré et respecté  Barton se révèle en fait sujet à des formes aiguës de dépression  sans doute atteint de trouble maniaco-dépressif[1]. Après trois divorces successifs  il finit par épouser la compositrice et musicienne française Germaine Tailleferre. Mais  Barton étant jaloux du talent de son épouse et redoutant le succès \"quelle\" pourrait avoir  \"soppose\" à sa carrière musicale  ce qui rendra leur vie de couple impossible et leur mariage ne durera que trois ans[2] [3]. En réalité  il ne peut se remettre de sa séparation en 1926 avec sa troisième épouse  \"lactrice\" et réalisatrice Carlotta Monterey (en).Le 19 mai 1931  dans son appartement situé à Manhattan  il met fin à ses jours en se tirant une balle dans la tempe.Barton tombe relativement assez vite dans \"loubli.En\" 1968  \"lÉcole\" de design de Rhode Island organise une exposition autour de ses dessins. En 1998  la National Portrait Gallery (Washington D.C.)  qui possède un fonds Barton  et la Bibliothèque du Congrès lui rendent un important hommage[4]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Biography_clean_brackets[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(Biography_clean_brackets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stique',\n",
       " 'modifier',\n",
       " 'modifier',\n",
       " 'le',\n",
       " 'code',\n",
       " 'artiste',\n",
       " 'peinture',\n",
       " 'sculpture',\n",
       " 'photographie',\n",
       " 'video',\n",
       " 'il',\n",
       " 'enseigne',\n",
       " 'dans',\n",
       " 'differentes',\n",
       " 'ecoles',\n",
       " 'de',\n",
       " 'tel',\n",
       " 'aviv',\n",
       " 'depuis',\n",
       " 'il',\n",
       " 'est',\n",
       " 'recompense',\n",
       " 'par',\n",
       " 'le',\n",
       " 'prix',\n",
       " 'du',\n",
       " 'ministere',\n",
       " 'de',\n",
       " 'leducation',\n",
       " 'et',\n",
       " 'de',\n",
       " 'la',\n",
       " 'culture',\n",
       " 'et',\n",
       " 'le',\n",
       " 'prix',\n",
       " 'du',\n",
       " 'musee',\n",
       " 'dart',\n",
       " 'de',\n",
       " 'petah',\n",
       " 'tikva',\n",
       " 'ses',\n",
       " 'œuvres',\n",
       " 'sont',\n",
       " 'presentees',\n",
       " 'dans',\n",
       " 'differents',\n",
       " 'musees',\n",
       " 'dont',\n",
       " 'le',\n",
       " 'musee',\n",
       " 'disrael',\n",
       " 'jerusalem',\n",
       " 'mort',\n",
       " 'modifier',\n",
       " 'modifier',\n",
       " 'le',\n",
       " 'code',\n",
       " 'le',\n",
       " 'er',\n",
       " 'fevrier',\n",
       " 'le',\n",
       " 'site',\n",
       " 'dinformations',\n",
       " 'mako',\n",
       " 'revele',\n",
       " 'quil',\n",
       " 'est',\n",
       " 'vise',\n",
       " 'par',\n",
       " 'une',\n",
       " 'enquete',\n",
       " 'concernant',\n",
       " 'des',\n",
       " 'relations',\n",
       " 'sexuelles',\n",
       " 'quil',\n",
       " 'aurait',\n",
       " 'eues',\n",
       " 'avec',\n",
       " 'des',\n",
       " 'eleves',\n",
       " 'mineures',\n",
       " 'le',\n",
       " 'lendemain',\n",
       " 'il',\n",
       " 'est',\n",
       " 'retrouve',\n",
       " 'mort',\n",
       " 'chez',\n",
       " 'lui',\n",
       " 'apparemment',\n",
       " 'suicide']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_words[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mateg\\Anaconda3\\lib\\site-packages\\gensim\\models\\phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['couve', 'braga', 'et', 'clemencia', 'rioseco', 'fernandez', 'adolfo', 'couve', 'est', 'laine', 'de', 'trois', 'freres', 'ne', 'valparaiso', 'en', 'il', 'passe', 'son', 'enfance', 'llay', 'llay', 'ou', 'il', 'habite', 'jusqua', 'lage', 'de', 'huit', 'ans', 'avant', 'de', 'sinstaller', 'avec', 'sa', 'famille', 'santiago', 'de', 'chile', 'il', 'acheve', 'sa', 'scolarite', 'au', 'colegio', 'san', 'ignacio', 'en', 'et', 'enterprend', 'ses', 'premieres', 'etudes', 'artistiques', 'lescuela', 'de', 'bellas', 'artes', 'de', 'luniversidad', 'de', 'chile', 'en', 'il', 'obtient', 'une', 'bourse', 'pour', 'etudier', 'lecole', 'des', 'beaux_arts', 'de', 'paris', 'et', 'lannee_suivante', 'poursuit', 'ses', 'etudes', 'aarts', 'student', 'league', 'new_york', 'il', 'ete', 'marie', 'avec', 'marta', 'carrasco', 'peintre', 'et', 'illustratrice', 'de', 'livres', 'pour', 'enfants', 'leur', 'fille', 'camila', 'couve', 'nee', 'en', 'fait', 'ses', 'debutes', 'dans', 'lecriture', 'en', 'en', 'publiant', 'estampas', 'de', 'nina', 'alfaguara', 'il', 'passe', 'les', 'dernieres_annees', 'de', 'sa', 'vie', 'carthagene', 'une', 'petite', 'ville', 'de', 'la', 'province', 'de', 'saint', 'antoine', 'apres', 'une', 'forte', 'depression', 'il', 'se', 'suicide', 'en']\n"
     ]
    }
   ],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-8b0609c95a8b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# Do lemmatization keeping only noun, adj, vb, adv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mdata_lemmatized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlemmatization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_words_bigrams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallowed_postags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'NOUN'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ADJ'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'VERB'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ADV'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m#print(stop_words)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-a9dac53f8cd2>\u001b[0m in \u001b[0;36mlemmatization\u001b[1;34m(texts, allowed_postags)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mtexts_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mtexts_out\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemma_\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mallowed_postags\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtexts_out\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nlp' is not defined"
     ]
    }
   ],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('fr', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "#print(stop_words)\n",
    "data_lemmatized = remove_stopwords(data_lemmatized)\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_lemmatized' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-b9409426f752>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Create Dictionary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mid2word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_lemmatized\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Create Corpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtexts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_lemmatized\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_lemmatized' is not defined"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
