{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('fr_core_news_md')\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from collections import Counter\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "from textblob import TextBlob\n",
    "from textblob_fr import PatternTagger, PatternAnalyzer\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('french'))\n",
    "stop_words.add('''['id=\"\"Biographie\"\">Biographie''')\n",
    "stop_words.add('''[modifier', '|', 'modifier', 'le', 'code]''')\n",
    "stop_words.add('''[\\'id=\"Biographie\">Biographie[modifier\\'''')\n",
    "stop_words.add('''|''')\n",
    "stop_words.add(''' ''')\n",
    "stop_words.add('''\\'''')\n",
    "stop_words.add('''(''')\n",
    "stop_words.add(''')''')\n",
    "stop_words.add('''d\\\\\\'''')\n",
    "stop_words.add('''id=\"Biographie\">Biographie[modifier''')\n",
    "stop_words.add('''[''')\n",
    "stop_words.add(''']''')\n",
    "stop_words.add('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '[',\n",
       " '[\\'id=\"\"Biographie\"\">Biographie',\n",
       " '[\\'id=\"Biographie\">Biographie[modifier\\'',\n",
       " \"[modifier', '|', 'modifier', 'le', 'code]\",\n",
       " ']',\n",
       " 'ai',\n",
       " 'aie',\n",
       " 'aient',\n",
       " 'aies',\n",
       " 'ait',\n",
       " 'as',\n",
       " 'au',\n",
       " 'aura',\n",
       " 'aurai',\n",
       " 'auraient',\n",
       " 'aurais',\n",
       " 'aurait',\n",
       " 'auras',\n",
       " 'aurez',\n",
       " 'auriez',\n",
       " 'aurions',\n",
       " 'aurons',\n",
       " 'auront',\n",
       " 'aux',\n",
       " 'avaient',\n",
       " 'avais',\n",
       " 'avait',\n",
       " 'avec',\n",
       " 'avez',\n",
       " 'aviez',\n",
       " 'avions',\n",
       " 'avons',\n",
       " 'ayant',\n",
       " 'ayante',\n",
       " 'ayantes',\n",
       " 'ayants',\n",
       " 'ayez',\n",
       " 'ayons',\n",
       " 'c',\n",
       " 'ce',\n",
       " 'ces',\n",
       " 'd',\n",
       " \"d\\\\'\",\n",
       " 'dans',\n",
       " 'de',\n",
       " 'des',\n",
       " 'du',\n",
       " 'elle',\n",
       " 'en',\n",
       " 'es',\n",
       " 'est',\n",
       " 'et',\n",
       " 'eu',\n",
       " 'eue',\n",
       " 'eues',\n",
       " 'eurent',\n",
       " 'eus',\n",
       " 'eusse',\n",
       " 'eussent',\n",
       " 'eusses',\n",
       " 'eussiez',\n",
       " 'eussions',\n",
       " 'eut',\n",
       " 'eux',\n",
       " 'eûmes',\n",
       " 'eût',\n",
       " 'eûtes',\n",
       " 'furent',\n",
       " 'fus',\n",
       " 'fusse',\n",
       " 'fussent',\n",
       " 'fusses',\n",
       " 'fussiez',\n",
       " 'fussions',\n",
       " 'fut',\n",
       " 'fûmes',\n",
       " 'fût',\n",
       " 'fûtes',\n",
       " 'id=\"Biographie\">Biographie[modifier',\n",
       " 'il',\n",
       " 'ils',\n",
       " 'j',\n",
       " 'je',\n",
       " 'l',\n",
       " 'la',\n",
       " 'le',\n",
       " 'les',\n",
       " 'leur',\n",
       " 'lui',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'mais',\n",
       " 'me',\n",
       " 'mes',\n",
       " 'moi',\n",
       " 'mon',\n",
       " 'même',\n",
       " 'n',\n",
       " 'nan',\n",
       " 'ne',\n",
       " 'nos',\n",
       " 'notre',\n",
       " 'nous',\n",
       " 'on',\n",
       " 'ont',\n",
       " 'ou',\n",
       " 'par',\n",
       " 'pas',\n",
       " 'pour',\n",
       " 'qu',\n",
       " 'que',\n",
       " 'qui',\n",
       " 's',\n",
       " 'sa',\n",
       " 'se',\n",
       " 'sera',\n",
       " 'serai',\n",
       " 'seraient',\n",
       " 'serais',\n",
       " 'serait',\n",
       " 'seras',\n",
       " 'serez',\n",
       " 'seriez',\n",
       " 'serions',\n",
       " 'serons',\n",
       " 'seront',\n",
       " 'ses',\n",
       " 'soient',\n",
       " 'sois',\n",
       " 'soit',\n",
       " 'sommes',\n",
       " 'son',\n",
       " 'sont',\n",
       " 'soyez',\n",
       " 'soyons',\n",
       " 'suis',\n",
       " 'sur',\n",
       " 't',\n",
       " 'ta',\n",
       " 'te',\n",
       " 'tes',\n",
       " 'toi',\n",
       " 'ton',\n",
       " 'tu',\n",
       " 'un',\n",
       " 'une',\n",
       " 'vos',\n",
       " 'votre',\n",
       " 'vous',\n",
       " 'y',\n",
       " '|',\n",
       " 'à',\n",
       " 'étaient',\n",
       " 'étais',\n",
       " 'était',\n",
       " 'étant',\n",
       " 'étante',\n",
       " 'étantes',\n",
       " 'étants',\n",
       " 'étiez',\n",
       " 'étions',\n",
       " 'été',\n",
       " 'étée',\n",
       " 'étées',\n",
       " 'étés',\n",
       " 'êtes'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_sentences = [\n",
    " '[\\'id=\"\"Biographie\"\">Biographie',\n",
    " '[\\'id=\"Biographie\">Biographie[modifier\\'',\n",
    " \"[modifier', '|', 'modifier', 'le', 'code]\",\n",
    " \"[modifier\\', \\'|\\', \\'modifier\\', \\'le\\', \\'code]\",\n",
    " \"modifier\\', \\'|\\', \\'modifier\\', \\'le\\', \\'code]\",\n",
    " '\"',\n",
    "'<span',\n",
    "' ','[',']','.',',','|',\n",
    " \"'\",\n",
    " '(',\n",
    " ')',\n",
    " '|',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '[',\n",
       " '[\\'id=\"\"Biographie\"\">Biographie',\n",
       " '[\\'id=\"Biographie\">Biographie[modifier\\'',\n",
       " \"[modifier', '|', 'modifier', 'le', 'code]\",\n",
       " ']',\n",
       " 'ai',\n",
       " 'aie',\n",
       " 'aient',\n",
       " 'aies',\n",
       " 'ait',\n",
       " 'as',\n",
       " 'au',\n",
       " 'aura',\n",
       " 'aurai',\n",
       " 'auraient',\n",
       " 'aurais',\n",
       " 'aurait',\n",
       " 'auras',\n",
       " 'aurez',\n",
       " 'auriez',\n",
       " 'aurions',\n",
       " 'aurons',\n",
       " 'auront',\n",
       " 'aux',\n",
       " 'avaient',\n",
       " 'avais',\n",
       " 'avait',\n",
       " 'avec',\n",
       " 'avez',\n",
       " 'aviez',\n",
       " 'avions',\n",
       " 'avons',\n",
       " 'ayant',\n",
       " 'ayante',\n",
       " 'ayantes',\n",
       " 'ayants',\n",
       " 'ayez',\n",
       " 'ayons',\n",
       " 'c',\n",
       " 'ce',\n",
       " 'ces',\n",
       " 'd',\n",
       " \"d\\\\'\",\n",
       " 'dans',\n",
       " 'de',\n",
       " 'des',\n",
       " 'du',\n",
       " 'elle',\n",
       " 'en',\n",
       " 'es',\n",
       " 'est',\n",
       " 'et',\n",
       " 'eu',\n",
       " 'eue',\n",
       " 'eues',\n",
       " 'eurent',\n",
       " 'eus',\n",
       " 'eusse',\n",
       " 'eussent',\n",
       " 'eusses',\n",
       " 'eussiez',\n",
       " 'eussions',\n",
       " 'eut',\n",
       " 'eux',\n",
       " 'eûmes',\n",
       " 'eût',\n",
       " 'eûtes',\n",
       " 'furent',\n",
       " 'fus',\n",
       " 'fusse',\n",
       " 'fussent',\n",
       " 'fusses',\n",
       " 'fussiez',\n",
       " 'fussions',\n",
       " 'fut',\n",
       " 'fûmes',\n",
       " 'fût',\n",
       " 'fûtes',\n",
       " 'id=\"Biographie\">Biographie[modifier',\n",
       " 'il',\n",
       " 'ils',\n",
       " 'j',\n",
       " 'je',\n",
       " 'l',\n",
       " 'la',\n",
       " 'le',\n",
       " 'les',\n",
       " 'leur',\n",
       " 'lui',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'mais',\n",
       " 'me',\n",
       " 'mes',\n",
       " 'moi',\n",
       " 'mon',\n",
       " 'même',\n",
       " 'n',\n",
       " 'nan',\n",
       " 'ne',\n",
       " 'nos',\n",
       " 'notre',\n",
       " 'nous',\n",
       " 'on',\n",
       " 'ont',\n",
       " 'ou',\n",
       " 'par',\n",
       " 'pas',\n",
       " 'pour',\n",
       " 'qu',\n",
       " 'que',\n",
       " 'qui',\n",
       " 's',\n",
       " 'sa',\n",
       " 'se',\n",
       " 'sera',\n",
       " 'serai',\n",
       " 'seraient',\n",
       " 'serais',\n",
       " 'serait',\n",
       " 'seras',\n",
       " 'serez',\n",
       " 'seriez',\n",
       " 'serions',\n",
       " 'serons',\n",
       " 'seront',\n",
       " 'ses',\n",
       " 'soient',\n",
       " 'sois',\n",
       " 'soit',\n",
       " 'sommes',\n",
       " 'son',\n",
       " 'sont',\n",
       " 'soyez',\n",
       " 'soyons',\n",
       " 'suis',\n",
       " 'sur',\n",
       " 't',\n",
       " 'ta',\n",
       " 'te',\n",
       " 'tes',\n",
       " 'toi',\n",
       " 'ton',\n",
       " 'tu',\n",
       " 'un',\n",
       " 'une',\n",
       " 'vos',\n",
       " 'votre',\n",
       " 'vous',\n",
       " 'y',\n",
       " '|',\n",
       " 'à',\n",
       " 'étaient',\n",
       " 'étais',\n",
       " 'était',\n",
       " 'étant',\n",
       " 'étante',\n",
       " 'étantes',\n",
       " 'étants',\n",
       " 'étiez',\n",
       " 'étions',\n",
       " 'été',\n",
       " 'étée',\n",
       " 'étées',\n",
       " 'étés',\n",
       " 'êtes'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('data/Suicides_wikipedia_scrapping_fr.csv')\n",
    "#data=pd.read_csv('BiographieParParagraphe.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data=pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'Link', 'Content']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'modifier le code Carrière artistique Artiste multidisciplinaire peinture sculpture photographie vidéo il enseigne dans différentes écoles de Tel Aviv depuis 1993 Il est récompensé par le prix du Ministère de l Éducation et de la Culture 2003 et le prix du musée d art de Petah Tikva 2006 Ses œuvres sont présentées dans différents musées dont le musée d Israël à Jérusalem Mort Le 1er février 2018 le site d informations Mako révèle qu il est visé par une enquête concernant des relations sexuelles qu il aurait eues avec des élèves mineures Le lendemain il est retrouvé mort chez lui apparemment suicidé'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Content'][2]=data['Content'][1]\n",
    "for j in range(len(stop_sentences)):\n",
    "    data['Content'][2]=re.sub('\\[[\\d]\\]','',data['Content'][2])\n",
    "    data['Content'][2]=data['Content'][2].replace(stop_sentences[j],' ')\n",
    "data['Content'][2]=' '.join(data['Content'][2].split())\n",
    "data['Content'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'modifier le code Carrière artistique Artiste multidisciplinaire peinture sculpture photographie vidéo il enseigne dans différentes écoles de Tel Aviv depuis 1993 Il est récompensé par le prix du Ministère de l Éducation et de la Culture 2003 et le prix du musée d art de Petah Tikva 2006 Ses œuvres sont présentées dans différents musées dont le musée d Israël à Jérusalem Mort Le 1er février 2018 le site d informations Mako révèle qu il est visé par une enquête concernant des relations sexuelles qu il aurait eues avec des élèves mineures Le lendemain il est retrouvé mort chez lui apparemment suicidé'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Content'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data['Content'])):\n",
    "    for j in range(len(stop_sentences)):\n",
    "        data['Content'][i]=re.sub('\\[[\\d]\\]','',data['Content'][i])\n",
    "        data['Content'][i]=data['Content'][i].replace(stop_sentences[j],' ')\n",
    "    data['Content'][i]=' '.join(data['Content'][i].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'modifier le code Carrière artistique Artiste multidisciplinaire peinture sculpture photographie vidéo il enseigne dans différentes écoles de Tel Aviv depuis 1993 Il est récompensé par le prix du Ministère de l Éducation et de la Culture 2003 et le prix du musée d art de Petah Tikva 2006 Ses œuvres sont présentées dans différents musées dont le musée d Israël à Jérusalem Mort Le 1er février 2018 le site d informations Mako révèle qu il est visé par une enquête concernant des relations sexuelles qu il aurait eues avec des élèves mineures Le lendemain il est retrouvé mort chez lui apparemment suicidé'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Content'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Content_Lemmatiser']=data['Content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Content_Lemmatiser=[el.split(' ') for el in data.Content_Lemmatiser]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['modifier',\n",
       " 'le',\n",
       " 'code',\n",
       " 'Markwitz',\n",
       " 'suivre',\n",
       " 'un',\n",
       " 'formation',\n",
       " 'pratique',\n",
       " 'et',\n",
       " 'artisanal',\n",
       " 'et',\n",
       " 'être',\n",
       " 'recommander',\n",
       " 'dans',\n",
       " 'le',\n",
       " 'cinéma',\n",
       " 'comme',\n",
       " 'charpentier',\n",
       " 'talentueux',\n",
       " 'à',\n",
       " 'le',\n",
       " 'fin',\n",
       " 'de',\n",
       " '1931',\n",
       " 'il',\n",
       " 'commence',\n",
       " 'à',\n",
       " 'travailler',\n",
       " 'à',\n",
       " 'titre',\n",
       " 'mineur',\n",
       " 'pour',\n",
       " 'un',\n",
       " 'production',\n",
       " 'cinématographique',\n",
       " 'même',\n",
       " 'à',\n",
       " 'Hollywood',\n",
       " 'au',\n",
       " 'début',\n",
       " 'The',\n",
       " 'Crowd',\n",
       " 'roar',\n",
       " 'alia',\n",
       " 'the',\n",
       " 'Doctor',\n",
       " 'en',\n",
       " '1934',\n",
       " 'il',\n",
       " 'être',\n",
       " 'autoriser',\n",
       " 'à',\n",
       " 'participer',\n",
       " 'pour',\n",
       " 'le',\n",
       " 'premier',\n",
       " 'fois',\n",
       " 'à',\n",
       " 'le',\n",
       " 'création',\n",
       " 'de',\n",
       " 'bâtiment',\n",
       " 'de',\n",
       " 'cinéma',\n",
       " 'en',\n",
       " 'tant',\n",
       " 'qu',\n",
       " 'assister',\n",
       " 'de',\n",
       " 'production',\n",
       " 'et',\n",
       " 'être',\n",
       " 'assigner',\n",
       " 'à',\n",
       " 'son',\n",
       " 'collègue',\n",
       " 'Heinrich',\n",
       " 'richter',\n",
       " 'et',\n",
       " 'Hans',\n",
       " 'jacoby',\n",
       " 'le',\n",
       " 'première',\n",
       " 'œuvre',\n",
       " 'de',\n",
       " 'Markwitz',\n",
       " 'être',\n",
       " 'deux',\n",
       " 'production',\n",
       " 'd',\n",
       " 'Erich',\n",
       " 'Engels',\n",
       " 'avec',\n",
       " 'Karl',\n",
       " 'Valentin',\n",
       " 'et',\n",
       " 'Liesl',\n",
       " 'Karlstadt',\n",
       " 'dans',\n",
       " 'le',\n",
       " 'rôle',\n",
       " 'principal',\n",
       " 'peu',\n",
       " 'de',\n",
       " 'temps',\n",
       " 'après',\n",
       " 'le',\n",
       " 'déclenchement',\n",
       " 'de',\n",
       " 'le',\n",
       " 'second',\n",
       " 'guerre',\n",
       " 'mondial',\n",
       " 'en',\n",
       " '1939',\n",
       " 'Markwitz',\n",
       " 'être',\n",
       " 'mobiliser',\n",
       " 'après',\n",
       " 'son',\n",
       " 'libération',\n",
       " 'Paul',\n",
       " 'Markwitz',\n",
       " 'rentre',\n",
       " 'à',\n",
       " 'Berlin',\n",
       " 'en',\n",
       " '1948',\n",
       " 'et',\n",
       " 'reprendre',\n",
       " 'son',\n",
       " 'travail',\n",
       " 'dans',\n",
       " 'le',\n",
       " 'cinéma',\n",
       " 'il',\n",
       " 'réalise',\n",
       " 'le',\n",
       " 'projet',\n",
       " 'de',\n",
       " 'collègue',\n",
       " 'important',\n",
       " 'comme',\n",
       " 'fritz',\n",
       " 'Maurischat',\n",
       " 'Ernst',\n",
       " 'heure',\n",
       " 'albrecht',\n",
       " 'Emil',\n",
       " 'Hasler',\n",
       " 'et',\n",
       " 'Heinrich',\n",
       " 'Weidemann',\n",
       " 'de',\n",
       " 'son',\n",
       " 'domaine',\n",
       " 'd',\n",
       " 'activité',\n",
       " 'être',\n",
       " 'pour',\n",
       " 'le',\n",
       " 'plupart',\n",
       " 'un',\n",
       " 'film',\n",
       " 'de',\n",
       " 'divertissement',\n",
       " 'peu',\n",
       " 'important',\n",
       " 'en',\n",
       " 'particulier',\n",
       " 'un',\n",
       " 'comédie',\n",
       " 'un',\n",
       " 'opérette',\n",
       " 'et',\n",
       " 'un',\n",
       " 'revue',\n",
       " 'musicale',\n",
       " 'mais',\n",
       " 'aussi',\n",
       " 'quelque',\n",
       " 'film',\n",
       " 'plus',\n",
       " 'ambitieux',\n",
       " 'et',\n",
       " 'dramatique',\n",
       " 'tel',\n",
       " 'que',\n",
       " 'le',\n",
       " 'production',\n",
       " 'américaine',\n",
       " 'tournée',\n",
       " 'dans',\n",
       " 'le',\n",
       " 'sud',\n",
       " 'de',\n",
       " 'l',\n",
       " 'Allemagne',\n",
       " 'le',\n",
       " 'diable',\n",
       " 'fait',\n",
       " 'le',\n",
       " 'troisième',\n",
       " 'en',\n",
       " 'avec',\n",
       " 'gene',\n",
       " 'kelly',\n",
       " 'et',\n",
       " 'le',\n",
       " 'récit',\n",
       " 'un',\n",
       " 'événement',\n",
       " 'entourer',\n",
       " 'le',\n",
       " 'tentative',\n",
       " 'd',\n",
       " 'assassinat',\n",
       " 'rater',\n",
       " 'de',\n",
       " 'hitler',\n",
       " 'en',\n",
       " '1944',\n",
       " 'c',\n",
       " 'être',\n",
       " 'arriver',\n",
       " 'le',\n",
       " '20',\n",
       " 'juillet',\n",
       " 'Markwitz',\n",
       " 'travaille',\n",
       " 'pour',\n",
       " 'un',\n",
       " 'grand',\n",
       " 'variété',\n",
       " 'de',\n",
       " 'société',\n",
       " 'de',\n",
       " 'production',\n",
       " 'à',\n",
       " 'le',\n",
       " 'fin',\n",
       " 'à',\n",
       " 'partir',\n",
       " 'de',\n",
       " '1957',\n",
       " 'il',\n",
       " 's',\n",
       " 'agir',\n",
       " 'presque',\n",
       " 'exclusivement',\n",
       " 'pour',\n",
       " 'ccc',\n",
       " 'son',\n",
       " 'dernier',\n",
       " 'œuvre',\n",
       " 'être',\n",
       " 'un',\n",
       " 'production',\n",
       " 'commander',\n",
       " 'pour',\n",
       " 'CCC',\n",
       " 'à',\n",
       " 'l',\n",
       " 'âge',\n",
       " 'de',\n",
       " '59',\n",
       " 'an',\n",
       " 'Paul',\n",
       " 'Markwitz',\n",
       " 'mourir',\n",
       " 'alors',\n",
       " 'qu',\n",
       " 'il',\n",
       " 'être',\n",
       " 'chômeur',\n",
       " 'il',\n",
       " 'se',\n",
       " 'jette',\n",
       " 'dans',\n",
       " 'un',\n",
       " 'canal',\n",
       " 'de',\n",
       " 'Berlin',\n",
       " 'dans',\n",
       " 'l',\n",
       " 'intention',\n",
       " 'de',\n",
       " 'se',\n",
       " 'suicider',\n",
       " 'et',\n",
       " 'se',\n",
       " 'noie']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Content_Lemmatiser'][i-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.57%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-7baf6ad48e66>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Content_Lemmatiser'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Content_Lemmatiser'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Content_Lemmatiser'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Content_Lemmatiser'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemma_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m    433\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"__call__\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE003\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE005\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.__call__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.predict\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.greedy_parse\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\thinc\\neural\\_classes\\model.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    167\u001b[0m             \u001b[0mMust\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \"\"\"\n\u001b[1;32m--> 169\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\thinc\\neural\\_classes\\model.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m_parser_model.pyx\u001b[0m in \u001b[0;36mspacy.syntax._parser_model.ParserModel.begin_update\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_parser_model.pyx\u001b[0m in \u001b[0;36mspacy.syntax._parser_model.ParserStepModel.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\thinc\\neural\\_classes\\feed_forward.py\u001b[0m in \u001b[0;36mbegin_update\u001b[1;34m(self, X, drop)\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\thinc\\api.py\u001b[0m in \u001b[0;36mbegin_update\u001b[1;34m(seqs_in, drop)\u001b[0m\n\u001b[0;32m    293\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseqs_in\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mseqs_in\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbp_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseqs_in\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbp_layer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\thinc\\neural\\_classes\\feed_forward.py\u001b[0m in \u001b[0;36mbegin_update\u001b[1;34m(self, X, drop)\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\thinc\\neural\\_classes\\resnet.py\u001b[0m in \u001b[0;36mbegin_update\u001b[1;34m(self, X, drop)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbp_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\thinc\\neural\\_classes\\feed_forward.py\u001b[0m in \u001b[0;36mbegin_update\u001b[1;34m(self, X, drop)\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\thinc\\neural\\_classes\\layernorm.py\u001b[0m in \u001b[0;36mbegin_update\u001b[1;34m(self, X, drop)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchild\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackprop_child\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchild\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[0mbackprop_child\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\thinc\\neural\\_classes\\maxout.py\u001b[0m in \u001b[0;36mbegin_update\u001b[1;34m(self, X__bi, drop)\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdrop\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[0mdrop\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop_factor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m         \u001b[0moutput__boc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgemm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX__bi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrans2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m         \u001b[0moutput__boc\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnO\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnP\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[0moutput__boc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput__boc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput__boc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnO\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time=time.time()\n",
    "for i in range(len(data['Content_Lemmatiser'])):\n",
    "    data['Content_Lemmatiser'][i]=data['Content_Lemmatiser'][i]\n",
    "    print(str(i/len(data['Content_Lemmatiser'])*100)[:4]+'%')\n",
    "    for j in range(len(data['Content_Lemmatiser'][i])):\n",
    "        if data['Content_Lemmatiser'][i]!=['']:\n",
    "            doc = nlp(data['Content_Lemmatiser'][i][j])\n",
    "            data['Content_Lemmatiser'][i][j]=doc[0].lemma_\n",
    "        else:\n",
    "            data['Content_Lemmatiser'][i]=[]\n",
    "        \n",
    "    clear_output(wait=True)\n",
    "print(\"Temps d execution : %s secondes ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['code',\n",
       " 'Zusho',\n",
       " 'être',\n",
       " 'naître',\n",
       " 'dans',\n",
       " 'le',\n",
       " 'ville',\n",
       " 'autour',\n",
       " 'de',\n",
       " 'château',\n",
       " 'de',\n",
       " 'Kagoshima',\n",
       " 'en',\n",
       " '1776',\n",
       " 'il',\n",
       " 'être',\n",
       " 'le',\n",
       " 'fils',\n",
       " 'de',\n",
       " 'Motoaki',\n",
       " 'Kawasaki',\n",
       " 'un',\n",
       " 'samouraï',\n",
       " 'de',\n",
       " 'satsuma',\n",
       " 'à',\n",
       " 'l',\n",
       " 'âge',\n",
       " 'de',\n",
       " '12',\n",
       " 'an',\n",
       " 'il',\n",
       " 'être',\n",
       " 'adopter',\n",
       " 'par',\n",
       " 'kiyonobu',\n",
       " 'Zusho',\n",
       " ';',\n",
       " 'à',\n",
       " '22',\n",
       " 'an',\n",
       " 'il',\n",
       " 'être',\n",
       " 'envoyer',\n",
       " 'à',\n",
       " 'edo',\n",
       " 'pour',\n",
       " 'devenir',\n",
       " 'l',\n",
       " 'assister',\n",
       " 'de',\n",
       " 'thé',\n",
       " 'de',\n",
       " 'daimyo',\n",
       " 'retirer',\n",
       " 'de',\n",
       " 'satsuma',\n",
       " 'Shimazu',\n",
       " 'Shigehide',\n",
       " 'celui',\n",
       " 'reconnaître',\n",
       " 'le',\n",
       " 'talent',\n",
       " 'de',\n",
       " 'Zusho',\n",
       " 'et',\n",
       " 'lui',\n",
       " 'donner',\n",
       " 'plus',\n",
       " 'de',\n",
       " 'responsabilité',\n",
       " 'il',\n",
       " 'être',\n",
       " 'plus',\n",
       " 'tard',\n",
       " 'employer',\n",
       " 'par',\n",
       " 'le',\n",
       " 'daimyo',\n",
       " 'en',\n",
       " 'service',\n",
       " 'de',\n",
       " 'satsuma',\n",
       " 'Shimazu',\n",
       " 'Narioki',\n",
       " 'lui',\n",
       " 'servir',\n",
       " 'de',\n",
       " 'messager',\n",
       " 'et',\n",
       " 'de',\n",
       " 'magistrat',\n",
       " 'il',\n",
       " 'être',\n",
       " 'également',\n",
       " 'impliquer',\n",
       " 'dans',\n",
       " 'de',\n",
       " 'commerce',\n",
       " 'illégal',\n",
       " 'avec',\n",
       " 'le',\n",
       " 'Chine',\n",
       " 'via',\n",
       " 'le',\n",
       " 'île',\n",
       " 'Ryūkyū',\n",
       " 'en',\n",
       " '1832',\n",
       " 'il',\n",
       " 'être',\n",
       " 'élevé',\n",
       " 'au',\n",
       " 'statut',\n",
       " 'de',\n",
       " 'karō',\n",
       " 'et',\n",
       " 'recevoir',\n",
       " 'officiellement',\n",
       " 'ce',\n",
       " 'rang',\n",
       " 'six',\n",
       " 'an',\n",
       " 'plus',\n",
       " 'tard',\n",
       " 'il',\n",
       " 'participer',\n",
       " 'ainsi',\n",
       " 'à',\n",
       " 'un',\n",
       " 'réforme',\n",
       " 'économique',\n",
       " 'agricole',\n",
       " 'et',\n",
       " 'militaire',\n",
       " 'à',\n",
       " 'ce',\n",
       " 'époque',\n",
       " 'le',\n",
       " 'budget',\n",
       " 'de',\n",
       " 'domaine',\n",
       " 'de',\n",
       " 'satsuma',\n",
       " 'être',\n",
       " 'en',\n",
       " 'déficit',\n",
       " 'net',\n",
       " 'de',\n",
       " '5',\n",
       " 'million',\n",
       " 'de',\n",
       " 'ryō',\n",
       " 'afin',\n",
       " 'de',\n",
       " 'remedier',\n",
       " 'à',\n",
       " 'ce',\n",
       " 'situation',\n",
       " 'il',\n",
       " 'entamer',\n",
       " 'un',\n",
       " 'programme',\n",
       " 'de',\n",
       " 'réforme',\n",
       " 'administratif',\n",
       " 'et',\n",
       " 'agricole',\n",
       " 'et',\n",
       " 'se',\n",
       " 'faire',\n",
       " 'accorder',\n",
       " 'un',\n",
       " 'prêt',\n",
       " 'sans',\n",
       " 'intérêt',\n",
       " 'par',\n",
       " 'le',\n",
       " 'commerçant',\n",
       " 'de',\n",
       " 'satsuma',\n",
       " 'devant',\n",
       " 'être',\n",
       " 'rembourser',\n",
       " 'sur',\n",
       " '250',\n",
       " 'an',\n",
       " 'ainsi',\n",
       " 'ce',\n",
       " 'prêt',\n",
       " 'courir',\n",
       " 'jusqu',\n",
       " 'en',\n",
       " '2085',\n",
       " 'mais',\n",
       " 'après',\n",
       " 'le',\n",
       " 'dissolution',\n",
       " 'un',\n",
       " 'domaine',\n",
       " 'de',\n",
       " 'Japon',\n",
       " 'en',\n",
       " '1872',\n",
       " 'le',\n",
       " 'gouvernement',\n",
       " 'de',\n",
       " 'Meiji',\n",
       " 'déclarer',\n",
       " 'le',\n",
       " 'dette',\n",
       " 'nulle',\n",
       " 'Zusho',\n",
       " 'avoir',\n",
       " 'également',\n",
       " 'intensifier',\n",
       " 'le',\n",
       " 'commerce',\n",
       " 'illégal',\n",
       " 'avec',\n",
       " 'le',\n",
       " 'Chine',\n",
       " 'de',\n",
       " 'le',\n",
       " 'dynastie',\n",
       " 'Qing',\n",
       " 'il',\n",
       " 'placer',\n",
       " 'un',\n",
       " 'monopole',\n",
       " 'sur',\n",
       " 'le',\n",
       " 'commerce',\n",
       " 'local',\n",
       " 'de',\n",
       " 'sucre',\n",
       " 'et',\n",
       " 'augmenter',\n",
       " 'le',\n",
       " 'production',\n",
       " 'et',\n",
       " 'le',\n",
       " 'vente',\n",
       " 'en',\n",
       " '1840',\n",
       " 'le',\n",
       " 'budget',\n",
       " 'de',\n",
       " 'domaine',\n",
       " 'connaître',\n",
       " 'un',\n",
       " 'surplus',\n",
       " 'de',\n",
       " '2',\n",
       " '5',\n",
       " 'million',\n",
       " 'de',\n",
       " 'ryō',\n",
       " 'cependant',\n",
       " 'Zusho',\n",
       " 'ne',\n",
       " 's',\n",
       " 'entendre',\n",
       " 'pas',\n",
       " 'avec',\n",
       " 'le',\n",
       " 'successeur',\n",
       " 'de',\n",
       " 'Narioki',\n",
       " 'son',\n",
       " 'fils',\n",
       " 'aîner',\n",
       " 'Shimazu',\n",
       " 'Nariakira',\n",
       " 'le',\n",
       " 'demi-frère',\n",
       " 'd',\n",
       " 'Hisamatsu',\n",
       " 'Shimazu',\n",
       " 'ce',\n",
       " 'être',\n",
       " 'préféré',\n",
       " 'par',\n",
       " 'Narioki',\n",
       " 'et',\n",
       " 'Zusho',\n",
       " 'avoir',\n",
       " 'peur',\n",
       " 'de',\n",
       " 'l',\n",
       " 'intérêt',\n",
       " 'de',\n",
       " 'Nariakira',\n",
       " 'pour',\n",
       " 'ce',\n",
       " 'qui',\n",
       " 'venir',\n",
       " 'de',\n",
       " 'l',\n",
       " 'occident',\n",
       " 'comme',\n",
       " 'Shigehide',\n",
       " 'il',\n",
       " 'penser',\n",
       " 'que',\n",
       " 'cela',\n",
       " 'pouvoir',\n",
       " 'ruiner',\n",
       " 'le',\n",
       " 'domaine',\n",
       " 'alors',\n",
       " 'qu',\n",
       " 'il',\n",
       " 'avoir',\n",
       " 'travailler',\n",
       " 'si',\n",
       " 'dur',\n",
       " 'pour',\n",
       " 'rétablir',\n",
       " 'le',\n",
       " 'budget',\n",
       " 'Nariakira',\n",
       " 'pour',\n",
       " 'écarter',\n",
       " 'son',\n",
       " 'ennemi',\n",
       " 'politique',\n",
       " 'qu',\n",
       " 'être',\n",
       " 'Narioki',\n",
       " 'et',\n",
       " 'Zusho',\n",
       " 'révéla',\n",
       " 'discrètement',\n",
       " 'le',\n",
       " 'commerce',\n",
       " 'illégal',\n",
       " 'avec',\n",
       " 'le',\n",
       " 'Chine',\n",
       " 'au',\n",
       " 'rōjū',\n",
       " 'Abe',\n",
       " 'Masahiro',\n",
       " 'en',\n",
       " '1848',\n",
       " 'pendant',\n",
       " 'que',\n",
       " 'Zusho',\n",
       " 'être',\n",
       " 'à',\n",
       " 'edo',\n",
       " 'il',\n",
       " 'être',\n",
       " 'convoqué',\n",
       " 'pae',\n",
       " 'Abe',\n",
       " 'qui',\n",
       " 'enquêter',\n",
       " 'sur',\n",
       " 'ce',\n",
       " 'commerce',\n",
       " 'illicite',\n",
       " 'peu',\n",
       " 'de',\n",
       " 'temps',\n",
       " 'après',\n",
       " 'Zusho',\n",
       " 'décéder',\n",
       " 'soudainement',\n",
       " 'dans',\n",
       " 'l',\n",
       " 'un',\n",
       " 'un',\n",
       " 'résidence',\n",
       " 'de',\n",
       " 'satsuma',\n",
       " 'à',\n",
       " 'edo',\n",
       " 'on',\n",
       " 'pense',\n",
       " 'qu',\n",
       " 'il',\n",
       " 's',\n",
       " 'être',\n",
       " 'fait',\n",
       " 'seppuku',\n",
       " 'ou',\n",
       " 'qu',\n",
       " 'il',\n",
       " 's',\n",
       " 'être',\n",
       " 'empoisonner',\n",
       " 'pour',\n",
       " 'éviter',\n",
       " 'à',\n",
       " 'Narioki',\n",
       " 'de',\n",
       " 'futur',\n",
       " 'ennui',\n",
       " 'il',\n",
       " 'avoir',\n",
       " '73',\n",
       " 'an',\n",
       " 'après',\n",
       " 'son',\n",
       " 'mort',\n",
       " 'Nariaka',\n",
       " 'retirer',\n",
       " 'à',\n",
       " 'son',\n",
       " 'famille',\n",
       " 'son',\n",
       " 'statut',\n",
       " 'son',\n",
       " 'résidence',\n",
       " 'et',\n",
       " 'son',\n",
       " 'revenu',\n",
       " 'le',\n",
       " 'tombe',\n",
       " 'de',\n",
       " 'Zusho',\n",
       " 'se',\n",
       " 'trouve',\n",
       " 'au',\n",
       " 'temple',\n",
       " 'fukushōji',\n",
       " 'dans',\n",
       " 'le',\n",
       " 'ville',\n",
       " 'actuelle',\n",
       " 'de',\n",
       " 'Kagoshima']"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Content_Lemmatiser'][i][3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data['Content_Lemmatiser'])):\n",
    "    data['Content_Lemmatiser'][i]=data['Content_Lemmatiser'][i][3:]\n",
    "for i in range(len(data['Content_Lemmatiser_No_Stop_Words'])):\n",
    "    data['Content_Lemmatiser_No_Stop_Words'][i]=data['Content_Lemmatiser_No_Stop_Words'][i][2:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data['Content_Lemmatiser_No_Stop_Words'])):\n",
    "    data['Content_Lemmatiser_No_Stop_Words'][i]=[w for w in data['Content_Lemmatiser'][i] if w not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words.add('»')\n",
    "stop_words.add('«')\n",
    "stop_words.add('etc')\n",
    "stop_words.add(';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Content_Lemmatiser_No_Stop_Words']=data['Lemmatiser']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('Suicides_wikipedia_scrapping_fr_LEMMATISER.csv',encoding='utf-8',sep=';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
